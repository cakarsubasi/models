{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from research.object_detection.utils import dataset_util\n",
    "from research.object_detection.utils import visualization_utils as viz_utils\n",
    "from research.object_detection.utils import label_map_util\n",
    "\n",
    "import numpy as np \n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "#matplotlib.use('TkAgg')\n",
    "#import tkinter \n",
    "#import ast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "record_train = \"data/muscima/training.record\"\n",
    "\n",
    "class_map = \"mapping_all_classes.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_omr_dataset_from_tfrecord(filename: str):\n",
    "    dataset = tf.data.TFRecordDataset(filename)\n",
    "\n",
    "    dataset = dataset.map(\n",
    "        parse_omr_element\n",
    "        )\n",
    "\n",
    "    return dataset\n",
    "\n",
    "    \n",
    "def parse_omr_element(element):\n",
    "\n",
    "    data = {\n",
    "    'image/height': tf.io.FixedLenFeature([], tf.int64),\n",
    "    'image/width': tf.io.FixedLenFeature([], tf.int64),\n",
    "    'image/filename': tf.io.FixedLenFeature([], tf.string),\n",
    "    'image/source_id': tf.io.FixedLenFeature([], tf.string),\n",
    "    #'image/key/sha256':tf.io.FixedLenFeature([], tf.string),\n",
    "    'image/encoded': tf.io.FixedLenFeature([], tf.string),\n",
    "    #'image/encoded': tf.io.decode_png([], channels=0),\n",
    "    'image/format': tf.io.FixedLenFeature([], tf.string),\n",
    "    'image/object/bbox/xmin': tf.io.FixedLenSequenceFeature([], tf.float32, allow_missing=True),\n",
    "    'image/object/bbox/xmax': tf.io.FixedLenSequenceFeature([], tf.float32, allow_missing=True),\n",
    "    'image/object/bbox/ymin': tf.io.FixedLenSequenceFeature([], tf.float32, allow_missing=True),\n",
    "    'image/object/bbox/ymax': tf.io.FixedLenSequenceFeature([], tf.float32, allow_missing=True),\n",
    "    'image/object/class/text': tf.io.FixedLenSequenceFeature([], tf.string, allow_missing=True),\n",
    "    'image/object/class/label': tf.io.FixedLenSequenceFeature([], tf.int64, allow_missing=True),\n",
    "    }\n",
    "\n",
    "    example = tf.io.parse_single_example(element, data)\n",
    "\n",
    "    example['image/decoded'] = tf.io.decode_image(example.get('image/encoded'))\n",
    "\n",
    "    example_np_xmin = example['image/object/bbox/xmin']\n",
    "    example_np_xmax = example['image/object/bbox/xmax']\n",
    "    example_np_ymin = example['image/object/bbox/ymin']\n",
    "    example_np_ymax = example['image/object/bbox/ymax']\n",
    "    \n",
    "    example_np_bbox = tf.stack((example_np_ymin, example_np_xmin, example_np_ymax, example_np_xmax))\n",
    "\n",
    "    example['image/bbox'] = tf.transpose(example_np_bbox)\n",
    "\n",
    "    # get rid of irrelevant objects\n",
    "    example.pop('image/encoded')\n",
    "    example.pop('image/format')\n",
    "    example.pop('image/object/bbox/xmin')\n",
    "    example.pop('image/object/bbox/ymin')\n",
    "    example.pop('image/object/bbox/xmax')\n",
    "    example.pop('image/object/bbox/ymax')\n",
    "\n",
    "    return example\n",
    "\n",
    "\n",
    "def plot_detections(image_np,\n",
    "                    boxes,\n",
    "                    classes, \n",
    "                    scores, \n",
    "                    category_index, \n",
    "                    figsize=(12,16),\n",
    "                    image_name=None):\n",
    "    image_np_with_annotations = image_np.copy()\n",
    "    image_np_with_annotations = np.repeat(image_np_with_annotations, 3, 2)\n",
    "    viz_utils.visualize_boxes_and_labels_on_image_array(\n",
    "        image_np_with_annotations,\n",
    "        boxes,\n",
    "        classes,\n",
    "        scores,\n",
    "        category_index,\n",
    "        use_normalized_coordinates=True,\n",
    "        min_score_thresh=0.8,\n",
    "        max_boxes_to_draw=scores.shape[0]\n",
    "    )\n",
    "    if image_name:\n",
    "        plt.imsave(image_name, image_np_with_annotations)\n",
    "    else:\n",
    "        plt.imshow(image_np_with_annotations)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_dataset = generate_omr_dataset_from_tfrecord(record_train)\n",
    "sample = training_dataset.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Attempt to convert a value (<TakeDataset element_spec={'image/filename': TensorSpec(shape=(), dtype=tf.string, name=None), 'image/height': TensorSpec(shape=(), dtype=tf.int64, name=None), 'image/object/class/label': TensorSpec(shape=(None,), dtype=tf.int64, name=None), 'image/object/class/text': TensorSpec(shape=(None,), dtype=tf.string, name=None), 'image/source_id': TensorSpec(shape=(), dtype=tf.string, name=None), 'image/width': TensorSpec(shape=(), dtype=tf.int64, name=None), 'image/decoded': TensorSpec(shape=<unknown>, dtype=tf.uint8, name=None), 'image/bbox': TensorSpec(shape=(None, 4), dtype=tf.float32, name=None)}>) with an unsupported type (<class 'tensorflow.python.data.ops.dataset_ops.TakeDataset'>) to a Tensor.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\explo\\Documents\\Repositories\\models\\Week 4.ipynb Cell 5'\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/explo/Documents/Repositories/models/Week%204.ipynb#ch0000015?line=0'>1</a>\u001b[0m data2 \u001b[39m=\u001b[39m {\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/explo/Documents/Repositories/models/Week%204.ipynb#ch0000015?line=1'>2</a>\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mimage/height\u001b[39m\u001b[39m'\u001b[39m: tf\u001b[39m.\u001b[39mio\u001b[39m.\u001b[39mFixedLenFeature([], tf\u001b[39m.\u001b[39mint64),\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/explo/Documents/Repositories/models/Week%204.ipynb#ch0000015?line=2'>3</a>\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mimage/width\u001b[39m\u001b[39m'\u001b[39m: tf\u001b[39m.\u001b[39mio\u001b[39m.\u001b[39mFixedLenFeature([], tf\u001b[39m.\u001b[39mint64),\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/explo/Documents/Repositories/models/Week%204.ipynb#ch0000015?line=3'>4</a>\u001b[0m }\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/explo/Documents/Repositories/models/Week%204.ipynb#ch0000015?line=5'>6</a>\u001b[0m example \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39;49mio\u001b[39m.\u001b[39;49mparse_single_example(sample, data2)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    <a href='file:///~/AppData/Roaming/Python/Python39/site-packages/tensorflow/python/util/traceback_utils.py?line=150'>151</a>\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    <a href='file:///~/AppData/Roaming/Python/Python39/site-packages/tensorflow/python/util/traceback_utils.py?line=151'>152</a>\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[1;32m--> <a href='file:///~/AppData/Roaming/Python/Python39/site-packages/tensorflow/python/util/traceback_utils.py?line=152'>153</a>\u001b[0m   \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[0;32m    <a href='file:///~/AppData/Roaming/Python/Python39/site-packages/tensorflow/python/util/traceback_utils.py?line=153'>154</a>\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m    <a href='file:///~/AppData/Roaming/Python/Python39/site-packages/tensorflow/python/util/traceback_utils.py?line=154'>155</a>\u001b[0m   \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\framework\\constant_op.py:102\u001b[0m, in \u001b[0;36mconvert_to_eager_tensor\u001b[1;34m(value, ctx, dtype)\u001b[0m\n\u001b[0;32m    <a href='file:///~/AppData/Roaming/Python/Python39/site-packages/tensorflow/python/framework/constant_op.py?line=99'>100</a>\u001b[0m     dtype \u001b[39m=\u001b[39m dtypes\u001b[39m.\u001b[39mas_dtype(dtype)\u001b[39m.\u001b[39mas_datatype_enum\n\u001b[0;32m    <a href='file:///~/AppData/Roaming/Python/Python39/site-packages/tensorflow/python/framework/constant_op.py?line=100'>101</a>\u001b[0m ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[1;32m--> <a href='file:///~/AppData/Roaming/Python/Python39/site-packages/tensorflow/python/framework/constant_op.py?line=101'>102</a>\u001b[0m \u001b[39mreturn\u001b[39;00m ops\u001b[39m.\u001b[39;49mEagerTensor(value, ctx\u001b[39m.\u001b[39;49mdevice_name, dtype)\n",
      "\u001b[1;31mValueError\u001b[0m: Attempt to convert a value (<TakeDataset element_spec={'image/filename': TensorSpec(shape=(), dtype=tf.string, name=None), 'image/height': TensorSpec(shape=(), dtype=tf.int64, name=None), 'image/object/class/label': TensorSpec(shape=(None,), dtype=tf.int64, name=None), 'image/object/class/text': TensorSpec(shape=(None,), dtype=tf.string, name=None), 'image/source_id': TensorSpec(shape=(), dtype=tf.string, name=None), 'image/width': TensorSpec(shape=(), dtype=tf.int64, name=None), 'image/decoded': TensorSpec(shape=<unknown>, dtype=tf.uint8, name=None), 'image/bbox': TensorSpec(shape=(None, 4), dtype=tf.float32, name=None)}>) with an unsupported type (<class 'tensorflow.python.data.ops.dataset_ops.TakeDataset'>) to a Tensor."
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'image/filename': TensorSpec(shape=(), dtype=tf.string, name=None),\n",
       " 'image/height': TensorSpec(shape=(), dtype=tf.int64, name=None),\n",
       " 'image/object/class/label': TensorSpec(shape=(None,), dtype=tf.int64, name=None),\n",
       " 'image/object/class/text': TensorSpec(shape=(None,), dtype=tf.string, name=None),\n",
       " 'image/source_id': TensorSpec(shape=(), dtype=tf.string, name=None),\n",
       " 'image/width': TensorSpec(shape=(), dtype=tf.int64, name=None),\n",
       " 'image/decoded': TensorSpec(shape=<unknown>, dtype=tf.uint8, name=None),\n",
       " 'image/bbox': TensorSpec(shape=(None, 4), dtype=tf.float32, name=None)}"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_dataset.element_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['image/encoded', 'image/filename', 'image/format', 'image/height', 'image/object/bbox/xmax', 'image/object/bbox/xmin', 'image/object/bbox/ymax', 'image/object/bbox/ymin', 'image/object/class/label', 'image/object/class/text', 'image/source_id', 'image/width', 'image/decoded'])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = tf.data.TFRecordDataset(record_train)\n",
    "\n",
    "\n",
    "data = {\n",
    "    'image/height': tf.io.FixedLenFeature([], tf.int64),\n",
    "    'image/width': tf.io.FixedLenFeature([], tf.int64),\n",
    "    'image/filename': tf.io.FixedLenFeature([], tf.string),\n",
    "    'image/source_id': tf.io.FixedLenFeature([], tf.string),\n",
    "    #'image/key/sha256':tf.io.FixedLenFeature([], tf.string),\n",
    "    'image/encoded': tf.io.FixedLenFeature([], tf.string),\n",
    "    #'image/encoded': tf.io.decode_png([], channels=0),\n",
    "    'image/format': tf.io.FixedLenFeature([], tf.string),\n",
    "    'image/object/bbox/xmin': tf.io.FixedLenSequenceFeature([], tf.float32, allow_missing=True),\n",
    "    'image/object/bbox/xmax': tf.io.FixedLenSequenceFeature([], tf.float32, allow_missing=True),\n",
    "    'image/object/bbox/ymin': tf.io.FixedLenSequenceFeature([], tf.float32, allow_missing=True),\n",
    "    'image/object/bbox/ymax': tf.io.FixedLenSequenceFeature([], tf.float32, allow_missing=True),\n",
    "    'image/object/class/text': tf.io.FixedLenSequenceFeature([], tf.string, allow_missing=True),\n",
    "    'image/object/class/label': tf.io.FixedLenSequenceFeature([], tf.int64, allow_missing=True),\n",
    "}\n",
    "\n",
    "training_data = []\n",
    "\n",
    "for element in dataset.as_numpy_iterator():\n",
    "    #example = tf.train.Example.FromString(element)\n",
    "    example = tf.io.parse_single_example(element, data)\n",
    "    training_data = [training_data, example]\n",
    "    break\n",
    "\n",
    "example.keys()\n",
    "example['image/decoded'] = tf.io.decode_image(example.get('image/encoded'))\n",
    "\n",
    "example.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'16th_flag': 1,\n",
       " '16th_rest': 2,\n",
       " '8th_flag': 3,\n",
       " '8th_rest': 4,\n",
       " 'accent': 5,\n",
       " 'arpeggio_wobble': 6,\n",
       " 'beam': 7,\n",
       " 'breath_mark': 8,\n",
       " 'c-clef': 9,\n",
       " 'curved-line_(tie-or-slur)': 10,\n",
       " 'dotted_horizontal_spanner': 11,\n",
       " 'double_sharp': 12,\n",
       " 'duration-dot': 13,\n",
       " 'dynamics_text': 14,\n",
       " 'f-clef': 15,\n",
       " 'fermata': 16,\n",
       " 'flat': 17,\n",
       " 'g-clef': 18,\n",
       " 'glissando': 19,\n",
       " 'grace-notehead-full': 20,\n",
       " 'grace_strikethrough': 21,\n",
       " 'hairpin-cresc': 22,\n",
       " 'hairpin-decr': 23,\n",
       " 'half_rest': 24,\n",
       " 'horizontal_spanner': 25,\n",
       " 'instrument_specific': 26,\n",
       " 'key_signature': 27,\n",
       " 'ledger_line': 28,\n",
       " 'letter_A': 29,\n",
       " 'letter_C': 30,\n",
       " 'letter_E': 31,\n",
       " 'letter_F': 32,\n",
       " 'letter_L': 33,\n",
       " 'letter_M': 34,\n",
       " 'letter_P': 35,\n",
       " 'letter_R': 36,\n",
       " 'letter_S': 37,\n",
       " 'letter_T': 38,\n",
       " 'letter_V': 39,\n",
       " 'letter_a': 40,\n",
       " 'letter_b': 41,\n",
       " 'letter_c': 42,\n",
       " 'letter_d': 43,\n",
       " 'letter_e': 44,\n",
       " 'letter_f': 45,\n",
       " 'letter_g': 46,\n",
       " 'letter_i': 47,\n",
       " 'letter_j': 48,\n",
       " 'letter_l': 49,\n",
       " 'letter_m': 50,\n",
       " 'letter_n': 51,\n",
       " 'letter_o': 52,\n",
       " 'letter_other': 53,\n",
       " 'letter_p': 54,\n",
       " 'letter_r': 55,\n",
       " 'letter_s': 56,\n",
       " 'letter_t': 57,\n",
       " 'letter_u': 58,\n",
       " 'letter_v': 59,\n",
       " 'letter_x': 60,\n",
       " 'letter_z': 61,\n",
       " 'measure_separator': 62,\n",
       " 'multi-measure_rest': 63,\n",
       " 'multi-staff_brace': 64,\n",
       " 'multi-staff_bracket': 65,\n",
       " 'multiple-note_tremolo': 66,\n",
       " 'natural': 67,\n",
       " 'notehead-empty': 68,\n",
       " 'notehead-full': 69,\n",
       " 'numeral_0': 70,\n",
       " 'numeral_1': 71,\n",
       " 'numeral_2': 72,\n",
       " 'numeral_3': 73,\n",
       " 'numeral_4': 74,\n",
       " 'numeral_5': 75,\n",
       " 'numeral_6': 76,\n",
       " 'numeral_7': 77,\n",
       " 'numeral_8': 78,\n",
       " 'ornament(s)': 79,\n",
       " 'other-clef': 80,\n",
       " 'other-dot': 81,\n",
       " 'other_numeric_sign': 82,\n",
       " 'other_text': 83,\n",
       " 'quarter_rest': 84,\n",
       " 'repeat': 85,\n",
       " 'repeat-dot': 86,\n",
       " 'repeat_measure': 87,\n",
       " 'sharp': 88,\n",
       " 'slur': 89,\n",
       " 'staccato-dot': 90,\n",
       " 'staff': 91,\n",
       " 'staff_grouping': 92,\n",
       " 'staff_line': 93,\n",
       " 'staff_space': 94,\n",
       " 'stem': 95,\n",
       " 'tempo_text': 96,\n",
       " 'tenuto': 97,\n",
       " 'thick_barline': 98,\n",
       " 'thin_barline': 99,\n",
       " 'tie': 100,\n",
       " 'time_signature': 101,\n",
       " 'transposition_text': 102,\n",
       " 'trill': 103,\n",
       " 'trill_wobble': 104,\n",
       " 'tuple': 105,\n",
       " 'tuple_bracketline': 106,\n",
       " 'unclassified': 107,\n",
       " 'volta': 108,\n",
       " 'whole-time_mark': 109,\n",
       " 'whole_rest': 110}"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_map_dict = label_map_util.get_label_map_dict(class_map)\n",
    "\n",
    "#for keys in label_map_dict.items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.16079545 0.68107647 0.2125     0.683653  ]\n",
      " [0.16647728 0.70684224 0.20568182 0.70941883]\n",
      " [0.15965909 0.7540796  0.21022727 0.75665617]\n",
      " ...\n",
      " [0.54829544 0.06269682 0.6153409  0.9587747 ]\n",
      " [0.68125    0.06298311 0.74886364 0.9584884 ]\n",
      " [0.81363636 0.06241054 0.8823864  0.9582021 ]]\n"
     ]
    }
   ],
   "source": [
    "img_raw = example['image/decoded'].numpy()\n",
    "\n",
    "#bboxes = [example['image/object/bbox/xmin'].numpy(), example['image/object/bbox/xmax'].numpy(),\n",
    "#          example['image/object/bbox/ymin'].numpy(), example['image/object/bbox/ymax'].numpy()] \n",
    "\n",
    "example_np_xmin = example['image/object/bbox/xmin'].numpy()\n",
    "example_np_xmax = example['image/object/bbox/xmax'].numpy()\n",
    "example_np_ymin = example['image/object/bbox/ymin'].numpy()\n",
    "example_np_ymax = example['image/object/bbox/ymax'].numpy()\n",
    "\n",
    "example_np_bbox = np.vstack((example_np_ymin, example_np_xmin, example_np_ymax, example_np_xmax)).transpose()\n",
    "\n",
    "example_np_labels = example['image/object/class/label'].numpy()\n",
    "\n",
    "example_np_bbox = np.vstack((example_np_ymin, example_np_xmin, example_np_ymax, example_np_xmax)).transpose()\n",
    "\n",
    "print(example_np_bbox)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "plt.imshow(img_raw_rgb)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1760, 3493, 3)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_raw_rgb = np.repeat(img_raw, 3, axis=2)\n",
    "img_raw_rgb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_scores = np.repeat(np.array([1.0], dtype=np.float32), example_np_labels.shape[0])\n",
    "\n",
    "plot_detections(img_raw_rgb, example_np_bbox, example_np_labels, dummy_scores, label_map_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95,\n",
       "       95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95,\n",
       "       95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95,\n",
       "       95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95,\n",
       "       95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95,\n",
       "       95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95,\n",
       "       95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95,\n",
       "       95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95,\n",
       "       95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95,\n",
       "       95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95,\n",
       "       95, 95, 95, 89, 89, 89, 89, 89, 89, 89, 89, 89, 89, 89, 89, 89, 89,\n",
       "       89, 89, 89, 95, 95, 95, 95,  7,  7,  7,  7,  7,  7,  7,  7,  7,  7,\n",
       "        7,  7,  7,  7,  7,  7,  7,  7,  7,  7,  7,  7,  7,  7,  7,  7,  7,\n",
       "        7,  7,  7,  7,  7,  7,  7,  7,  7,  7,  7,  7,  7,  7,  7,  7,  7,\n",
       "        7,  7, 89, 89, 89, 95, 95, 95,  7,  7, 95,  7,  7, 95, 95,  7,  7,\n",
       "        7,  7,  7, 95,  7, 91, 91, 91, 91, 91, 91], dtype=int64)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_np_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_detections(image_np,\n",
    "                    boxes,\n",
    "                    classes, \n",
    "                    scores, \n",
    "                    category_index, \n",
    "                    figsize=(12,16),\n",
    "                    image_name=None):\n",
    "    image_np_with_annotations = image_np.copy()\n",
    "    viz_utils.visualize_boxes_and_labels_on_image_array(\n",
    "        image_np_with_annotations,\n",
    "        boxes,\n",
    "        classes,\n",
    "        scores,\n",
    "        category_index,\n",
    "        use_normalized_coordinates=True,\n",
    "        min_score_thresh=0.8,\n",
    "        max_boxes_to_draw=scores.shape[0]\n",
    "    )\n",
    "    if image_name:\n",
    "        plt.imsave(image_name, image_np_with_annotations)\n",
    "    else:\n",
    "        plt.imshow(image_np_with_annotations)\n",
    "        plt.show()\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "73d43add95346d4398e31ae9f91d6e09eefd54bad8c0b6b9d72d8d059bb26101"
  },
  "kernelspec": {
   "display_name": "Python 3.9.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
